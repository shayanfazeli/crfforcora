{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing and Statistical Overview of CORA Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = './data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import _pickle as pickle\n",
    "import pdb\n",
    "import re\n",
    "\n",
    "xml_begin = re.compile('<(\\w+)>')\n",
    "xml_end = re.compile('</(\\w+)>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a table of output citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "with open(dataset_path + 'citations', 'r') as fid:\n",
    "    for f in fid.readlines():\n",
    "        output.append(f.strip().replace('\\t', ','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_citations = {}\n",
    "with open(dataset_path + 'citations', 'r') as fid:\n",
    "    for f in fid.readlines():\n",
    "        current_line = f.strip().split()\n",
    "        current_line = [int(current_line[0]),int(current_line[1])]\n",
    "        if current_line[0] not in output_citations:\n",
    "            output_citations[current_line[0]] = []\n",
    "        output_citations[current_line[0]].append(current_line[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get IDs of all the documents cited by document id 16 you run the following command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_citations[16] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('number of keys: {}'.format(len(output_citations.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a table of ps -> class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifications = {}\n",
    "\n",
    "with open(dataset_path + 'classifications', 'r') as fid:\n",
    "    for f in fid.readlines():\n",
    "        current_line = f.strip().split()\n",
    "        if len(current_line) == 0:\n",
    "            continue\n",
    "        if current_line[0] == \"keywords\":\n",
    "            continue\n",
    "        classifications[current_line[0]] = current_line[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = set(classifications.values())\n",
    "print(labels)\n",
    "print(classifications['http:##www.isi.edu#sims#papers#94-sims-agents.ps'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to processing xml info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_xml_info_in_papers_file(xml_info):\n",
    "    output = {}\n",
    "    missing = 'missing'\n",
    "    tag = missing\n",
    "    output[tag] = []\n",
    "    tokens = xml_info.split()\n",
    "    for token in tokens:\n",
    "        tag_begin = xml_begin.match(token)\n",
    "        tag_end = xml_end.match(token)\n",
    "\n",
    "        if tag_begin:\n",
    "            tag = tag_begin.group(1)\n",
    "            output[tag] = []\n",
    "        elif tag_end:\n",
    "            assert tag_end.group(1) == tag, 'Bad XML nesting: %s != %s' % (tag_end.group(1), tag)\n",
    "            tag = missing\n",
    "        else:\n",
    "            token = re.sub(r'\\W+', '', token)\n",
    "            token = token.lower()\n",
    "            if len(token) > 0:\n",
    "                output[tag].append(token)\n",
    "    if 'address' in output.keys():\n",
    "        output['address'] = [''.join(output['address'])]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_paper_features_from_processed_xml_info(input_information):\n",
    "    input_information\n",
    "    features = []\n",
    "\n",
    "    # preprocessings:\n",
    "    if 'author' in input_information.keys():\n",
    "        input_information['author'] = [e for e in input_information['author'] if len(e) > 2]\n",
    "\n",
    "    input_information['title'] = [e for e in input_information['title'] if len(e) > 3]\n",
    "    if 'author' in input_information.keys():\n",
    "        for i in range(max(3, len(input_information['author']))):\n",
    "            if i >= len(input_information['author']):\n",
    "                features.append(\n",
    "                    'author%d=missing' % i\n",
    "                )\n",
    "            else:\n",
    "                features.append(\n",
    "                    'author{}={}'.format(i, input_information['author'][i])\n",
    "                )\n",
    "    else:\n",
    "        for i in range(3):\n",
    "            features.append(\n",
    "                    'author%d=missing' % i\n",
    "                )\n",
    "            \n",
    "    for i in range(max(6, len(input_information['title']))):\n",
    "        if i >= len(input_information['title']):\n",
    "            features.append(\n",
    "                'title%d=missing' % i\n",
    "            )\n",
    "        else:\n",
    "            features.append(\n",
    "                'title{}={}'.format(i, input_information['title'][i])\n",
    "            )\n",
    "\n",
    "    if 'publisher' in input_information.keys():   \n",
    "        features.append(\n",
    "            'publisher={}'.format(input_information['publisher'][0])\n",
    "        )\n",
    "    else:\n",
    "        features.append(\n",
    "            'publisher=missing'\n",
    "        )\n",
    "\n",
    "    if 'address' in input_information.keys():   \n",
    "        features.append(\n",
    "            'address={}'.format(input_information['address'][0])\n",
    "        )\n",
    "    else:\n",
    "        features.append(\n",
    "            'address=missing'\n",
    "        )\n",
    "    if 'year' in input_information.keys():   \n",
    "        features.append(\n",
    "            'year={}'.format(input_information['year'][0])\n",
    "        )\n",
    "    else:\n",
    "        features.append(\n",
    "            'year=missing'\n",
    "        ) \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a table for id -> features, id -> ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = {}\n",
    "features_for_id = {}\n",
    "raw_xml_info_for_id = {}\n",
    "with open(dataset_path + 'papers', 'r') as fid:\n",
    "    for f in fid.readlines():\n",
    "        elements = f.strip().split('\\t')\n",
    "        if int(elements[0]) in papers:\n",
    "            papers[int(elements[0])].append(elements[1])\n",
    "            continue\n",
    "        papers[int(elements[0])] = [elements[1]]\n",
    "        if len(elements) == 3:\n",
    "            raw_xml_info_for_id[int(elements[0])] = elements[2]\n",
    "            features_for_id[int(elements[0])] = extract_paper_features_from_processed_xml_info(\n",
    "                process_xml_info_in_papers_file(elements[2])\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers[18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features_for_id[18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_xml_info_for_id[18]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-structuring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dataset = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_for_id = {}\n",
    "\n",
    "for pid in papers.keys():\n",
    "    for ps_file in papers[pid]:\n",
    "        if ps_file in classifications:\n",
    "            label_for_id[pid] = classifications[ps_file]\n",
    "            break\n",
    "    if pid not in label_for_id:\n",
    "        label_for_id[pid] = 'NOLABEL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dataset['postscript_for_id'] = papers\n",
    "output_dataset['label_for_id'] = label_for_id\n",
    "output_dataset['edges'] = output_citations\n",
    "output_dataset['features_for_id'] = features_for_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(output_dataset, open(dataset_path + 'restructured_dataset_v2.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(output_dataset['postscript_for_id']))\n",
    "print(len(output_dataset['label_for_id']))\n",
    "print(len(output_dataset['edges']))\n",
    "print(len(output_dataset['features_for_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output_dataset['postscript_for_id'][18])\n",
    "print(output_dataset['label_for_id'][18])\n",
    "print(output_dataset['edges'][18])\n",
    "print(output_dataset['features_for_id'][18])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
