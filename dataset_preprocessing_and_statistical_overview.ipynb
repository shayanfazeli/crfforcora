{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing and Statistical Overview of CORA Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = './data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import _pickle as pickle\n",
    "import pdb\n",
    "import re\n",
    "\n",
    "xml_begin = re.compile('<(\\w+)>')\n",
    "xml_end = re.compile('</(\\w+)>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "with open(dataset_path + 'citations', 'r') as fid:\n",
    "    for f in fid.readlines():\n",
    "        output.append(f.strip().replace('\\t', ','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_citations = {}\n",
    "for element in output:\n",
    "    if int(element.split(',')[0]) not in output_citations.keys():\n",
    "        output_citations[int(element.split(',')[0])] = []\n",
    "    output_citations[int(element.split(',')[0])].append(int(element.split(',')[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('number of keys: {}'.format(len(output_citations.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how many papers (unique paper ids) have citations (are cited by >0 papers)\n",
    "print('number of unique citeds (values): %d' % len(set([item for sublist in output_citations.values() for item in sublist]))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifications = []\n",
    "with open(dataset_path + 'classifications', 'r') as fid:\n",
    "    for f in fid.readlines():\n",
    "        classifications.append(f.strip().replace('\\t', ','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = (set([k.split(',')[1] for k in classifications[:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demoing the values in the classification list, including the classes\n",
    "classifications[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_classes_for_ps = {}\n",
    "with open(dataset_path + 'classifications', 'r') as fid:\n",
    "    for f in fid.readlines():\n",
    "        string_values = f.strip().split('\\t') ### BAD\n",
    "        if len(string_values) > 1:\n",
    "            pruned_classes_for_ps[string_values[0]] = string_values[1]\n",
    "            \n",
    "pruned_classes_for_ps['ftp:##ftp.eecs.umich.edu#people#fessler#ps#93,isit,hero.ps.Z'] # outputs mscs.ps.Z which is wrong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing xml info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_xml_info_in_papers_file(xml_info):\n",
    "    output = {}\n",
    "    missing = 'missing'\n",
    "    tag = missing\n",
    "    output[tag] = []\n",
    "    tokens = xml_info.split()\n",
    "    for token in tokens:\n",
    "        tag_begin = xml_begin.match(token)\n",
    "        tag_end = xml_end.match(token)\n",
    "\n",
    "        if tag_begin:\n",
    "            tag = tag_begin.group(1)\n",
    "            output[tag] = []\n",
    "        elif tag_end:\n",
    "            assert tag_end.group(1) == tag, 'Bad XML nesting: %s != %s' % (tag_end.group(1), tag)\n",
    "            tag = missing\n",
    "        else:\n",
    "            token = re.sub(r'\\W+', '', token)\n",
    "            token = token.lower()\n",
    "            if len(token) > 0:\n",
    "                output[tag].append(token)\n",
    "    if 'address' in output.keys():\n",
    "        output['address'] = [''.join(output['address'])]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_paper_features_from_processed_xml_info(input_information):\n",
    "    input_information\n",
    "    features = []\n",
    "\n",
    "    # preprocessings:\n",
    "    if 'author' in input_information.keys():\n",
    "        input_information['author'] = [e for e in input_information['author'] if len(e) > 2]\n",
    "\n",
    "    input_information['title'] = [e for e in input_information['title'] if len(e) > 3]\n",
    "    if 'author' in input_information.keys():\n",
    "        for i in range(max(3, len(input_information['author']))):\n",
    "            if i >= len(input_information['author']):\n",
    "                features.append(\n",
    "                    'author%d=missing' % i\n",
    "                )\n",
    "            else:\n",
    "                features.append(\n",
    "                    'author{}={}'.format(i, input_information['author'][i])\n",
    "                )\n",
    "    else:\n",
    "        for i in range(3):\n",
    "            features.append(\n",
    "                    'author%d=missing' % i\n",
    "                )\n",
    "\n",
    "\n",
    "    \n",
    "    for i in range(max(6, len(input_information['title']))):\n",
    "        if i >= len(input_information['title']):\n",
    "            features.append(\n",
    "                'title%d=missing' % i\n",
    "            )\n",
    "        else:\n",
    "            features.append(\n",
    "                'title{}={}'.format(i, input_information['title'][i])\n",
    "            )\n",
    "\n",
    "    if 'publisher' in input_information.keys():   \n",
    "        features.append(\n",
    "            'publisher={}'.format(input_information['publisher'][0])\n",
    "        )\n",
    "    else:\n",
    "        features.append(\n",
    "            'publisher=missing'\n",
    "        )\n",
    "\n",
    "    if 'address' in input_information.keys():   \n",
    "        features.append(\n",
    "            'address={}'.format(input_information['address'][0])\n",
    "        )\n",
    "    else:\n",
    "        features.append(\n",
    "            'address=missing'\n",
    "        )\n",
    "    if 'year' in input_information.keys():   \n",
    "        features.append(\n",
    "            'year={}'.format(input_information['year'][0])\n",
    "        )\n",
    "    else:\n",
    "        features.append(\n",
    "            'year=missing'\n",
    "        ) \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = []\n",
    "features_for_id = {}\n",
    "raw_xml_info_for_id = {}\n",
    "with open(dataset_path + 'papers', 'r') as fid:\n",
    "    for f in fid.readlines():\n",
    "        elements = f.strip().split('\\t')\n",
    "        papers.append([int(elements[0]), elements[1]])\n",
    "        if len(elements) == 3:\n",
    "            raw_xml_info_for_id[int(elements[0])] = elements[2]\n",
    "            features_for_id[int(elements[0])] = extract_paper_features_from_processed_xml_info(\n",
    "                process_xml_info_in_papers_file(elements[2])\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: WHY IS THIS ASSERT FALSE? (according to the paper though, removing duplicates is good enough)\n",
    "# @shayan. What do you mean here? See papers[:5] and notice that papers actually has duplicates \n",
    "# and there haven't been removed yet\n",
    "\n",
    "# SHAYAN: this document is not presentation-ready, these were just my debuggings.\n",
    "len(set([f[0] for f in papers])) == len([f[0] for f in papers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(classifications[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features_for_id[16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_xml_info_for_id[16]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-structuring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dataset = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postscript_for_id = {}\n",
    "for element in papers:\n",
    "    postscript_for_id[element[0]] = element[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: because of the above todo to pruned_classes_for_ps there might be some changes here.\n",
    "#Even if no code, output definitely changes\n",
    "label_for_id = postscript_for_id\n",
    "for id_value in postscript_for_id.keys():\n",
    "    ps_file = postscript_for_id[id_value]\n",
    "    if ps_file in pruned_classes_for_ps.keys():\n",
    "        label_for_id[id_value] = pruned_classes_for_ps[ps_file]\n",
    "    else:\n",
    "        label_for_id[id_value] = 'NOLABEL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = output_citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dataset['postscript_for_id'] = postscript_for_id\n",
    "output_dataset['label_for_id'] = label_for_id\n",
    "output_dataset['edges'] = edges\n",
    "output_dataset['features_for_id'] = features_for_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(output_dataset, open(dataset_path + 'restructured_dataset.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges[16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(output_dataset['postscript_for_id']))\n",
    "print(len(output_dataset['label_for_id']))\n",
    "print(len(output_dataset['edges']))\n",
    "print(len(output_dataset['features_for_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output_dataset['postscript_for_id'][18])\n",
    "print(output_dataset['label_for_id'][18])\n",
    "print(output_dataset['edges'][18])\n",
    "print(output_dataset['features_for_id'][18])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
